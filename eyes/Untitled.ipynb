{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10c9787c-e663-4dd0-b9bd-dc483c2371dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "#-----Step 1: Use VideoCapture in OpenCV-----\n",
    "import cv2\n",
    "import dlib\n",
    "import math\n",
    "BLINK_RATIO_THRESHOLD = 5.7\n",
    "\n",
    "#-----Step 5: Getting to know blink ratio\n",
    "\n",
    "def midpoint(point1 ,point2):\n",
    "    return (point1.x + point2.x)/2,(point1.y + point2.y)/2\n",
    "\n",
    "def euclidean_distance(point1 , point2):\n",
    "    return math.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)\n",
    "\n",
    "def get_blink_ratio(eye_points, facial_landmarks):\n",
    "\n",
    "    #loading all the required points\n",
    "    corner_left  = (facial_landmarks.part(eye_points[0]).x,\n",
    "                    facial_landmarks.part(eye_points[0]).y)\n",
    "    corner_right = (facial_landmarks.part(eye_points[3]).x,\n",
    "                    facial_landmarks.part(eye_points[3]).y)\n",
    "\n",
    "    center_top    = midpoint(facial_landmarks.part(eye_points[1]),\n",
    "                             facial_landmarks.part(eye_points[2]))\n",
    "    center_bottom = midpoint(facial_landmarks.part(eye_points[5]),\n",
    "                             facial_landmarks.part(eye_points[4]))\n",
    "\n",
    "    #calculating distance\n",
    "    horizontal_length = euclidean_distance(corner_left,corner_right)\n",
    "    vertical_length = euclidean_distance(center_top,center_bottom)\n",
    "\n",
    "    ratio = horizontal_length / vertical_length\n",
    "\n",
    "    return ratio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac10eb57-4647-4272-84c9-7174e62329c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0] global /build/opencv/src/opencv-4.5.3/modules/videoio/src/cap_gstreamer.cpp (597) isPipelinePlaying OpenCV | GStreamer warning: GStreamer: pipeline have not been created\n"
     ]
    }
   ],
   "source": [
    "#livestream from the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "'''in case of a video\n",
    "cap = cv2.VideoCapture(\"__path_of_the_video__\")'''\n",
    "\n",
    "#name of the display window in OpenCV\n",
    "cv2.namedWindow('BlinkDetector')\n",
    "\n",
    "#-----Step 3: Face detection with dlib-----\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "#-----Step 4: Detecting Eyes using landmarks in dlib-----\n",
    "predictor = dlib.shape_predictor(\"data/shape_predictor_68_face_landmarks.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d57ee10-399a-417c-b707-c2bf0405552d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m           shape_predictor\n",
       "\u001b[0;31mString form:\u001b[0m    <_dlib_pybind11.shape_predictor object at 0x7fc0742c72b0>\n",
       "\u001b[0;31mFile:\u001b[0m           /usr/lib/python3.9/site-packages/_dlib_pybind11.cpython-39-x86_64-linux-gnu.so\n",
       "\u001b[0;31mDocstring:\u001b[0m      This object is a tool that takes in an image region containing some object and outputs a set of point locations that define the pose of the object. The classic example of this is human face pose prediction, where you take an image of a human face as input and are expected to identify the locations of important facial landmarks such as the corners of the mouth and eyes, tip of the nose, and so forth.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "__init__(*args, **kwargs)\n",
       "Overloaded function.\n",
       "\n",
       "1. __init__(self: _dlib_pybind11.shape_predictor) -> None\n",
       "\n",
       "2. __init__(self: _dlib_pybind11.shape_predictor, arg0: str) -> None\n",
       "\n",
       "Loads a shape_predictor from a file that contains the output of the \n",
       "train_shape_predictor() routine.\n",
       "\u001b[0;31mCall docstring:\u001b[0m\n",
       "__call__(self: _dlib_pybind11.shape_predictor, image: array, box: _dlib_pybind11.rectangle) -> _dlib_pybind11.full_object_detection\n",
       "\n",
       "requires \n",
       "    - image is a numpy ndarray containing either an 8bit grayscale or RGB \n",
       "      image. \n",
       "    - box is the bounding box to begin the shape prediction inside. \n",
       "ensures \n",
       "    - This function runs the shape predictor on the input image and returns \n",
       "      a single full_object_detection.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8c9859-09b6-4e15-9cd8-262479e33bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#these landmarks are based on the image above\n",
    "left_eye_landmarks  = [36, 37, 38, 39, 40, 41]\n",
    "right_eye_landmarks = [42, 43, 44, 45, 46, 47]\n",
    "\n",
    "while True:\n",
    "    #capturing frame\n",
    "    retval, frame = cap.read()\n",
    "\n",
    "    #exit the application if frame not found\n",
    "    if not retval:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "\n",
    "    #-----Step 2: converting image to grayscale-----\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #-----Step 3: Face detection with dlib-----\n",
    "    #detecting faces in the frame\n",
    "    faces,_,_ = detector.run(image = frame, upsample_num_times = 0,\n",
    "                       adjust_threshold = 0.0)\n",
    "    # print(f'{faces = }\\n{a = }\\n{b =}\\n{\"=\"*80}')\n",
    "\n",
    "    #-----Step 4: Detecting Eyes using landmarks in dlib-----\n",
    "    for face in faces:\n",
    "\n",
    "        landmarks = predictor(frame, face)\n",
    "\n",
    "        #-----Step 5: Calculating blink ratio for one eye-----\n",
    "        left_eye_ratio  = get_blink_ratio(left_eye_landmarks, landmarks)\n",
    "        right_eye_ratio = get_blink_ratio(right_eye_landmarks, landmarks)\n",
    "        blink_ratio     = (left_eye_ratio + right_eye_ratio) / 2\n",
    "        lr = left_eye_ratio/right_eye_ratio\n",
    "        # print(f'{left_eye_ratio = }')\n",
    "        # print(f'{right_eye_ratio = }')\n",
    "        # print(f'{blink_ratio = }')\n",
    "        # print(f'{lr = }')\n",
    "        # print('='*80)\n",
    "\n",
    "        # if blink_ratio > BLINK_RATIO_THRESHOLD:\n",
    "        #     #Blink detected! Do Something!\n",
    "        #     cv2.putText(frame,\"BLINKING\",(10,50), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        #                 2,(255,255,255),2,cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('BlinkDetector', frame)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "#releasing the VideoCapture object\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
